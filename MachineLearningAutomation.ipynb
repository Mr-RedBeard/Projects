{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH+nXQre5mRbgquVCxJ2Ni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-RedBeard/Projects/blob/main/MachineLearningAutomation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression model accuracy Automation**"
      ],
      "metadata": {
        "id": "ii9V3UPKJNKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from skopt import BayesSearchCV\n",
        "from mlxtend.regressor import StackingRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Define regression algorithms\n",
        "regression_algorithms = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Lasso Regression\": Lasso(),\n",
        "    \"Ridge Regression\": Ridge(),\n",
        "    \"ElasticNet Regression\": ElasticNet(),\n",
        "    \"Decision Tree Regression\": DecisionTreeRegressor(),\n",
        "    \"Random Forest Regression\": RandomForestRegressor(),\n",
        "    \"K-Nearest Neighbors Regression\": KNeighborsRegressor(),\n",
        "}\n",
        "\n",
        "# Define ensemble methods\n",
        "ensemble_models = {\n",
        "    \"Voting Regressor\": VotingRegressor(estimators=list(regression_algorithms.items())),\n",
        "    \"AdaBoost Regressor\": AdaBoostRegressor(),\n",
        "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(),\n",
        "}\n",
        "\n",
        "# Define scalers\n",
        "scalers = {\n",
        "    \"Standard Scaler\": StandardScaler(),\n",
        "    \"Min-Max Scaler\": MinMaxScaler(),\n",
        "    \"Robust Scaler\": RobustScaler()\n",
        "}\n",
        "\n",
        "# Define polynomial features\n",
        "poly = PolynomialFeatures()\n",
        "\n",
        "# Define regularization techniques\n",
        "regularization_params = {\n",
        "    Lasso(): {'alpha': (1e-6, 1e+6, 'log-uniform')},\n",
        "    Ridge(): {'alpha': (1e-6, 1e+6, 'log-uniform')},\n",
        "    ElasticNet(): {'alpha': (1e-6, 1e+6, 'log-uniform'), 'l1_ratio': (0, 1)}\n",
        "}\n",
        "\n",
        "# Define feature selection techniques\n",
        "feature_selection = {\n",
        "    \"Select From Model\": SelectFromModel(estimator=RandomForestRegressor()),\n",
        "    \"RFE\": RFE(estimator=LinearRegression()),\n",
        "    \"SelectKBest\": SelectKBest(score_func=f_regression),\n",
        "}\n",
        "\n",
        "# Define imputation techniques\n",
        "imputation_techniques = {\n",
        "    \"Mean\": SimpleImputer(strategy='mean'),\n",
        "    \"Median\": SimpleImputer(strategy='median'),\n",
        "    \"Most Frequent\": SimpleImputer(strategy='most_frequent')\n",
        "}\n",
        "\n",
        "# Combine all components for grid search\n",
        "regression_combinations = [\n",
        "    (scaler, model, ensemble, poly, reg, fs, imp)\n",
        "    for scaler in scalers.values()\n",
        "    for model in regression_algorithms.values()\n",
        "    for ensemble in ensemble_models.values()\n",
        "    for reg in regularization_params.items()\n",
        "    for fs in feature_selection.items()\n",
        "    for imp in imputation_techniques.items()\n",
        "]\n",
        "\n",
        "# Train and evaluate each combination with cross-validation and Bayesian optimization\n",
        "results = []\n",
        "for scaler, model, ensemble, poly, reg, fs, imp in regression_combinations:\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[('imputer', imp, slice(0, 10))],\n",
        "        remainder='passthrough')\n",
        "    pipeline = Pipeline(steps=[('scaler', scaler), ('poly', poly), ('fs', fs), ('preprocessor', preprocessor), ('model', model), ('ensemble', ensemble)])\n",
        "    bayes_search = BayesSearchCV(pipeline, reg, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, n_iter=50)\n",
        "    bayes_search.fit(X, y)\n",
        "    y_pred = bayes_search.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    results.append({\n",
        "        \"Scaler\": scaler.__class__.__name__,\n",
        "        \"Algorithm\": model.__class__.__name__,\n",
        "        \"Ensemble\": ensemble.__class__.__name__,\n",
        "        \"Regularization\": reg[0].__class__.__name__,\n",
        "        \"Feature_Selection\": fs[0],\n",
        "        \"Imputation\": imp[0],\n",
        "        \"MSE\": mse,\n",
        "        \"R^2\": r2\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for visualization\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(results_df.index, results_df[\"MSE\"])\n",
        "plt.xlabel(\"Configuration Index\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.title(\"Mean Squared Error Across Different Configurations\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WJwMwxdbFdRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification models Accuracy Automation**"
      ],
      "metadata": {
        "id": "Rm5M3mRLClEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skopt import BayesSearchCV\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Generate sample classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Define classification algorithms\n",
        "classification_algorithms = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbors Classifier\": KNeighborsClassifier(),\n",
        "}\n",
        "\n",
        "# Define ensemble methods\n",
        "ensemble_models = {\n",
        "    \"Voting Classifier\": VotingClassifier(estimators=list(classification_algorithms.items())),\n",
        "    \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "}\n",
        "\n",
        "# Define scalers\n",
        "scalers = {\n",
        "    \"Standard Scaler\": StandardScaler(),\n",
        "    \"Min-Max Scaler\": MinMaxScaler(),\n",
        "    \"Robust Scaler\": RobustScaler()\n",
        "}\n",
        "\n",
        "# Define polynomial features\n",
        "poly = PolynomialFeatures()\n",
        "\n",
        "# Define regularization techniques\n",
        "regularization_params = {\n",
        "    LogisticRegression(): {'C': (1e-6, 1e+6, 'log-uniform')},\n",
        "}\n",
        "\n",
        "# Define feature selection techniques\n",
        "feature_selection = {\n",
        "    \"Select From Model\": SelectFromModel(estimator=RandomForestClassifier()),\n",
        "    \"RFE\": RFE(estimator=LogisticRegression()),\n",
        "    \"SelectKBest\": SelectKBest(score_func=f_classif),\n",
        "}\n",
        "\n",
        "# Define imputation techniques\n",
        "imputation_techniques = {\n",
        "    \"Mean\": SimpleImputer(strategy='mean'),\n",
        "    \"Median\": SimpleImputer(strategy='median'),\n",
        "    \"Most Frequent\": SimpleImputer(strategy='most_frequent')\n",
        "}\n",
        "\n",
        "# Combine all components for grid search\n",
        "classification_combinations = [\n",
        "    (scaler, model, ensemble, poly, reg, fs, imp)\n",
        "    for scaler in scalers.values()\n",
        "    for model in classification_algorithms.values()\n",
        "    for ensemble in ensemble_models.values()\n",
        "    for reg in regularization_params.items()\n",
        "    for fs in feature_selection.items()\n",
        "    for imp in imputation_techniques.items()\n",
        "]\n",
        "\n",
        "# Train and evaluate each combination with cross-validation and Bayesian optimization\n",
        "results = []\n",
        "for scaler, model, ensemble, poly, reg, fs, imp in classification_combinations:\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[('imputer', imp, slice(0, 10))],\n",
        "        remainder='passthrough')\n",
        "    pipeline = Pipeline(steps=[('scaler', scaler), ('poly', poly), ('fs', fs), ('preprocessor', preprocessor), ('model', model), ('ensemble', ensemble)])\n",
        "    bayes_search = BayesSearchCV(pipeline, reg, cv=5, scoring='accuracy', n_jobs=-1, n_iter=50)\n",
        "    bayes_search.fit(X, y)\n",
        "    y_pred = bayes_search.predict(X)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    results.append({\n",
        "        \"Scaler\": scaler.__class__.__name__,\n",
        "        \"Algorithm\": model.__class__.__name__,\n",
        "        \"Ensemble\": ensemble.__class__.__name__,\n",
        "        \"Regularization\": reg[0].__class__.__name__,\n",
        "        \"Feature_Selection\": fs[0],\n",
        "        \"Imputation\": imp[0],\n",
        "        \"Accuracy\": accuracy\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame for visualization\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(results_df.index, results_df[\"Accuracy\"])\n",
        "plt.xlabel(\"Configuration Index\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy Across Different Configurations\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6njPvnqQFljN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}